---
title: "Boosted GLM"
output: html_notebook
---

### Create classification Target

Create $isbot \;\in \{0,1\}$ column based on botscore
```{r}
dataset$isbot <- F
suspended <- which(dataset$botscore == "suspended")
dataset$isbot[suspended] = T
protected <- which(dataset$botscore == "protected")
dataset$isbot[protected] = F
dataset$isbot <- factor(dataset$isbot)

botrows <- which(dataset$botscore > 0.5)
dataset$isbot[botrows] = T

# remove "botscore" column
dataset$botscore <- NULL

summary(dataset)
str(dataset)
```

Some plots
```{r}
# plot(dataset$influence, dataset$isbot)
# plot(dataset$mcsize, dataset$isbot)
# plot(dataset$verified, dataset$isbot)
# plot(dataset$tweetsCount, dataset$isbot)
# plot(dataset)
```


### Preprocess data with PCA
```{r}
# separate features and target
target <- dataset$isbot
features <- dataset
features$isbot <- NULL

pca <- preProcess(features, method = c("center", "scale", "pca"),
                  thresh = 0.95, pcaComp = NULL)
print(pca)
pca_transformed <- predict(pca, dataset)

summary(pca_transformed)
```

### Normalize by standardizing (0 mean and 1 stdev)

normalize PCA transformed features
```{r}
# separate features and target
target <- dataset$isbot
features <- dataset
features$isbot <- NULL

normalized <- normalize(features, method = "standardize", range = c(0, 1), 
                             on.constant = "quiet")

# put together processed feature columns and target
normalized <- cbind(target, normalized)

# rename target variable
colnames(normalized)[1] <- "isbot"

summary(normalized)
```


### normalized (scaled) dataset
```{r}
# separate features and target
ntarget <- data.frame(dataset$isbot)
nfeatures <- dataset

# catdata <- as.factor(cbind(dataset$verified, dataset$location.objectType))

#remove categorial and target data
nfeatures$isbot <- NULL
nfeatures$verified <- NULL
nfeatures$location.objectType <- NULL

# scale features to mean=0 and stdev=1
normalized <- scale(nfeatures)

normalized <- cbind(ntarget,normalized, dataset$verified, dataset$location.objectType)
# normalized <- data.frame(normalized)
colnames(normalized)[1] <- "isbot"
colnames(normalized)[13] <- "location.objectType"
colnames(normalized)[12] <- "verified"
```


Splitting Training set and Test set:
determine source of preprocessing
```{r}
data <- sample_n(tbl=normalized, size=5000, replace=F)

data$isbot <- as.character(data$isbot)
data$isbot[which(data$isbot == T)] <- "bot"
data$isbot[which(data$isbot == F)] <- "human"
data$isbot <- as.factor(data$isbot)


split_point <- 0.75*nrow(data)
data_train <- data[1:split_point,]
data_test <- data[(split_point+1):nrow(data),]
```


### Construct Neural Net

```{r}
library(nnet)
```


```{r}

nnctrl <- trainControl(method = 'cv', number = 10,
                         classProbs = TRUE, 
                         verboseIter = TRUE, 
                         summaryFunction = twoClassSummary, 
                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))


cmtr <- matrix(c(0,20,1,0))

mod2 <- train(isbot ~ ., data = data_train, 
              method = 'nnet',
              tuneGrid = expand.grid(size = c(6,4),
                                   decay = c(0.1),
                                   cost = cmtr
                                   ),
              weights = cmtr,
              trControl = nnctrl,
              verbose = FALSE)

mod2
```

```{r}
#Predicting using random forest model
model_c5_pred <- predict(mod2, data_test, type="raw")

pred_obs <- data.frame(predicted = model_c5_pred, observed = data_test$isbot)
confusionMatrix(data = model_c5_pred, reference = data_test$isbot)
```



